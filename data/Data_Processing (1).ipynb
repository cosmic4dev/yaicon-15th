{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Required Libraries**"
      ],
      "metadata": {
        "id": "ysBz4m5O1P4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXx5DYx7xj9D",
        "outputId": "1000ffdd-4585-436f-a160-169daf309088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/363.4 MB\u001b[0m \u001b[31m154.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#Installing Required Libraries\n",
        "!pip install transformers sentencepiece --upgrade\n",
        "!pip install torch\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use this code if the raw data is too big**"
      ],
      "metadata": {
        "id": "vL2oPVex1k3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this code if the raw data is too big\n",
        "import pandas as pd\n",
        "!pip install scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# file path of your raw data\n",
        "file_path = \"/root/data/train+test+valid.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# first split : 5: (3+2)\n",
        "part_1, part_tmp = train_test_split(df, test_size=0.5, random_state=42, shuffle=True)\n",
        "\n",
        "# second split : part_tmp → 3:2\n",
        "part_2, part_3 = train_test_split(part_tmp, test_size=0.4, random_state=42, shuffle=True)\n",
        "# (0.4 = 2 / (3+2))\n",
        "\n",
        "# saving\n",
        "part_1.to_csv(\"/root/data/50.csv\", index=False)\n",
        "part_2.to_csv(\"/root/data/30.csv\", index=False)\n",
        "part_3.to_csv(\"/root/data/20.csv\", index=False)\n",
        "\n",
        "print(\"Done spliting and saving !\")\n",
        "print(\"1.csv rows:\", len(part_1))\n",
        "print(\"2.csv rows:\", len(part_2))\n",
        "print(\"3.csv rows:\", len(part_3))\n"
      ],
      "metadata": {
        "id": "wQjJ2VnjyWJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Translationese Column to Our Raw Data**"
      ],
      "metadata": {
        "id": "c3sjVSm81rHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Translationese Column to Our Raw Data\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model and set device\n",
        "model_name = \"NHNDQ/nllb-finetuned-en2ko\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model on GPU with FP16 precision (only works on GPU)\n",
        "if device.type == \"cuda\":\n",
        "    model = model.half()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Fast batch translation function\n",
        "def translate_batch_fast(text_list, batch_size=1024):\n",
        "    results = []\n",
        "    tokenizer.src_lang = \"eng_Latn\"\n",
        "    bos_token_id = tokenizer.convert_tokens_to_ids(\"kor_Hang\")\n",
        "\n",
        "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
        "        batch = text_list[i:i+batch_size]\n",
        "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **encoded,\n",
        "                forced_bos_token_id=bos_token_id,\n",
        "                max_length=512,\n",
        "                num_beams=1,      # greedy decoding for speed\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "        results.extend(decoded)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load full dataset\n",
        "df = pd.read_csv(\"/root/data/20.csv\")\n",
        "text_list = df[\"en\"].fillna(\"\").tolist()\n",
        "\n",
        "# Run full translation\n",
        "translated_ko = translate_batch_fast(text_list, batch_size=128)\n",
        "\n",
        "# Append translation results and save\n",
        "df[\"translated_ko\"] = translated_ko\n",
        "output_path = \"/root/data/20_translated.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"Translation completed! Saved to:\", output_path)"
      ],
      "metadata": {
        "id": "k6v_glT2zZDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Filtering**"
      ],
      "metadata": {
        "id": "-YnF99Yy1x3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Filtering\n",
        "import pandas as pd\n",
        "import re\n",
        "from bert_score import score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====== 1. Load the CSV file ======\n",
        "file_path = \"/root/data/50_translated.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(f\" Original number of rows: {len(df)}\")\n",
        "\n",
        "# ====== 2. Filter by minimum sentence length ======\n",
        "min_len = len(\"내시경하는 기분이야.\")  # 11 characters\n",
        "before = len(df)\n",
        "df = df[df[\"ko\"].astype(str).apply(lambda x: len(x) >= min_len)]\n",
        "after = len(df)\n",
        "print(f\" Length filtering: {before - after} rows removed → {after} rows remaining\")\n",
        "\n",
        "# ====== 3. Filter by allowed characters ======\n",
        "# Allowed characters: Korean, English, numbers, whitespace, and selected symbols\n",
        "allowed_pattern = re.compile(r'^[\\uAC00-\\uD7A3a-zA-Z0-9\\s!@#$%^&*()_\\-+={}\\[\\]|\\\\:;\"\\'<>,.?/~`]+$')\n",
        "\n",
        "def is_valid(text):\n",
        "    return bool(allowed_pattern.fullmatch(str(text).strip()))\n",
        "\n",
        "before = len(df)\n",
        "mask_valid = df[[\"ko\", \"en\", \"translated_ko\"]].applymap(is_valid).all(axis=1)\n",
        "df = df[mask_valid].reset_index(drop=True)\n",
        "after = len(df)\n",
        "print(f\" Character filtering: {before - after} rows removed → {after} rows remaining\")\n",
        "\n",
        "# ====== 4. Compute BERTScore (F1) ======\n",
        "ko_texts = df[\"ko\"].astype(str).tolist()\n",
        "translated_texts = df[\"translated_ko\"].astype(str).tolist()\n",
        "batch_size = 1024\n",
        "f1_scores = []\n",
        "\n",
        "print(\" Calculating BERTScore F1 (using GPU + batching)...\")\n",
        "for i in tqdm(range(0, len(df), batch_size), desc=\"Progress\", unit=\"batch\"):\n",
        "    batch_ko = ko_texts[i:i+batch_size]\n",
        "    batch_trans = translated_texts[i:i+batch_size]\n",
        "    try:\n",
        "        _, _, F1 = score(batch_trans, batch_ko, lang='ko', device='cuda', verbose=False)\n",
        "        f1_scores.extend([f.item() for f in F1])\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n GPU memory error! Try reducing batch_size from {batch_size}.\")\n",
        "        raise e\n",
        "\n",
        "# ====== 5. Filter by F1 score threshold ======\n",
        "df[\"f1\"] = f1_scores\n",
        "before = len(df)\n",
        "df_filtered = df[df[\"f1\"] > 0.9].drop(columns=[\"f1\"])\n",
        "after = len(df_filtered)\n",
        "print(f\" BERTScore filtering (F1 ≤ 0.9): {before - after} rows removed → {after} rows remaining\")\n",
        "\n",
        "# ====== 6. Save the filtered results ======\n",
        "output_path = \"/root/data/50_translated_filtered.csv\"\n",
        "df_filtered.to_csv(output_path, index=False)\n",
        "\n",
        "# ====== 7. Summary output ======\n",
        "print(f\"\\n Final results saved!\")\n",
        "print(f\" {len(df_filtered)} out of {len(df)} rows remaining ({len(df_filtered)/len(df)*100:.2f}%)\")\n",
        "print(f\" Saved to: {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tmc0tEFU0HR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}